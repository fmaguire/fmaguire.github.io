<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Gavin Gray" />
  <title>Defying the Curse of Dimensionality: Competitive Seizure Prediction with Kaggle</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="reveal.js/css/reveal.min.css" />
    <style type="text/css">code{white-space: pre;}</style>
    <link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>
    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Defying the Curse of Dimensionality: Competitive Seizure Prediction with Kaggle</h1>
    <h2 class="author">Gavin Gray</h2>
    <h3 class="date">November 28th 2014</h3>
</section>

<section><section id="what-does-a-kaggle-competition-look-like" class="titleslide slide level1"><h1>What does a Kaggle competition look like?</h1></section><section id="the-view-from-github" class="slide level2">
<h1>The view from Github</h1>
<div class="fragment">
<figure>
<img src="images/hscommits.png" alt="Commits by time." /><figcaption>Commits by time.</figcaption>
</figure>
<aside class="notes">
<p>As many people here probably already know what a Kaggle competition is I'm going to describe what they are in a roundabout way that hopefully won't be boring to those people.</p>
<p>To get an idea of what something is, instead of describing it, what if I just show you what it looks like from some different angles? Then you can get an idea of what it is yourself. These are some ways of looking at the Kaggle competition project Scott, Finlay and I were working on.</p>
This is one of the standard graphs github will produce if you ask it to.
</aside>
</div>
</section><section class="slide level2">

<figure>
<img src="images/hspunchcard.png" alt="Punch card graph of hours when we were working." /><figcaption>Punch card graph of hours when we were working.</figcaption>
</figure>
<aside class="notes">
<p>Looking at the punchcard you can see when we were making most of our commits in this project.</p>
That might say more about us than it does about the project.
</aside>
</section><section id="the-view-from-the-data" class="slide level2">
<h1>The view from the data</h1>
<figure>
<img src="images/AESraw.png" alt="Graph showing example pre-ictal samples from the raw data (Kaggle Inc, 2014)." /><figcaption>Graph showing example pre-ictal samples from the raw data <span class="citation" data-cites="aeskaggle">(<span>Kaggle Inc</span>, 2014)</span>.</figcaption>
</figure>
<aside class="notes">
The data you can see here is what we were working with. These are recordings from electrodes inside the brains of patients and dogs in two states: preictal and interictal. i.e. just before a seizure and <em>not</em> just before a seizure.
</aside>
</section><section class="slide level2">

<figure>
<img src="images/F8.large.jpg" alt="Stand-in example spectrogram(Queiroz et al., 2009)." /><figcaption>Stand-in example spectrogram<span class="citation" data-cites="queiroz_dynamics_2009">(Queiroz et al., 2009)</span>.</figcaption>
</figure>
<aside class="notes">
These graphs show some of what was produced from the raw data. We ended up with a vast array of different pre-processing options I'll describe later in the presentation but this should give you an idea of what they are.
</aside>
</section><section class="slide level2">

<figure>
<img src="images/hsspectralclass.png" alt="Coloured by class." /><figcaption>Coloured by class.</figcaption>
</figure>
<aside class="notes">
<p>This is a spectral embedding.</p>
<blockquote>
<p>Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.</p>
</blockquote>
This graph illustrates why generalisation in this problem was so difficult. You can see that if our task was to tell the difference between test and training it would be much easier than telling the difference between interictal and preictal.
</aside>
</section><section class="slide level2">

<figure>
<img src="images/hsspectralhour.png" alt="Coloured by hour." /><figcaption>Coloured by hour.</figcaption>
</figure>
<aside class="notes">

</aside>
</section><section id="the-view-from-kaggle" class="slide level2">
<h1>The view from Kaggle</h1>
<figure>
<img src="images/hsleaderboard.png" alt="Final leaderboard results (Kaggle Inc, 2014)." /><figcaption>Final leaderboard results <span class="citation" data-cites="aeskaggle">(<span>Kaggle Inc</span>, 2014)</span>.</figcaption>
</figure>
<aside class="notes">
<p>So we try to predict whether the samples are preictal or interictal, then we supply our predictions on all of these test samples to Kaggle and they score us on the leader board.</p>
<p>That's the competition.</p>
<p>We were scored on a measure of area under the curve (AUC) on an receiver operating characteristic (ROC) curve. Basically, it's a plot of false positive versus true positive rate as the threshold of the classifier was varied.</p>
<p>We could submit ten entries a day, and receive scores on each. So you might think we could just overfit the test data by submitting a huge number of times. There's a catch, the score on the leaderboard <em>during</em> the competition is only calculated from a fraction of the submission, and at the end the real value using all of the test data is revealed.</p>
This means that at the end of the competition, there's a big shakeup of the scores. It worked pretty well for us, as we went up 15 places.
</aside>
</section></section>
<section><section id="what-can-you-work-on" class="titleslide slide level1"><h1>What can you work on?</h1></section><section id="historical-competitions" class="slide level2">
<h1>Historical competitions</h1>
<p>A sample from 143 completed competitions:</p>
<ul>
<li>Heritage Health Prize</li>
<li>Merck Molecular Activity Challenge</li>
<li>Observing Dark Worlds</li>
<li>The Marinexplore and Cornell University Whale Detection Challenge</li>
<li>Africa Soil Property Prediction Challenge</li>
<li>CONNECTOMICS (that is the whole name)</li>
<li>Many, many corporate competitions...</li>
</ul>
<aside class="notes">
<p>Examples of historical competitions go here.</p>
<p>I've tried to pick out a good mix of the different kinds of projects on there, but there's definitely some that are different from those I've picked.</p>
<p>The first one here, the Heritage Health Prize was a competition to predict whether someone would go to hospital based on their previous health problems in the last year. The prize was $500,000 (it's the biggest prize so far awarded).</p>
<p>The Observing Dark Worlds challenge I put there because <a href="http://homepages.inf.ed.ac.uk/imurray2/">Iain Murray</a> here at Edinburgh did very well in that one, getting 2nd (narrowly).</p>
The others are a mix. A large proportion of the competitions are corporate analytics: predicting if people will click adds, employee habits etc.
</aside>
</section><section id="tools" class="slide level2">
<h1>Tools</h1>
<p>Free to use anything to get the job done. We used:</p>
<ul>
<li>Matlab</li>
<li>Scikit-learn</li>
<li>Git</li>
<li>Various other Python packages</li>
<li>Working with HDF5s</li>
<li>MongoDB</li>
</ul>
<aside class="notes">
<p>Of course, in a Kaggle competition you're free to use any tools you want to use. You could use something that you just want to learn to use, or whatever your favourite tool for the job is.</p>
Here are some of the things we used. We used Matlab for feature preprocessing as the files came in .mat format, which makes them more difficult to open than the documentation for scipy.io says.
</aside>
</section><section id="techniques" class="slide level2">
<h1>Techniques</h1>
<p>It's possible to quickly try things out to see if they'll work.</p>
<h3 id="preprocessing">Preprocessing</h3>
<p>Comprehensive list of features can be found in the [repository][repo]. Useful extractions were:</p>
<ul>
<li><code>cln,csp,dwn_feat_pib_ratioBB_</code>:
<ul>
<li>cln - Cleaned</li>
<li>csp - Common Spatial Patterns (transformation)</li>
<li>dwn - Downsampled</li>
<li>pib - Power in band</li>
<li>ratioBB - ratio of power to broadband power</li>
</ul></li>
<li><code>cln,ica,dwn_feat_mvar-PDC_</code>:
<ul>
<li>ica - Independent Component Analysis (transformation)</li>
<li>mvar - coefficients of fitted Multivariate-AutoRegressive model</li>
<li>PDC - Partial Directed Coherence for MVAR</li>
</ul></li>
<li>And approximately 850 other options...</li>
</ul>
<aside class="notes">
<p>Here are two examples of pre-processing options Scott worked on. I've deciphered the naming convention to make it more clear what each is.</p>
<p>The first of these in the plots we've already seen. Despite being relatively simple, it was very effective, which is consistent with what other teams found. The other feature here also scored highly in our batch tests, but unfortunately we were never able to accurately reproduce those results on the leaderboard, which was frustrating.</p>
There are many, many more of these options which Scott managed to create, all are sitting on the salmon server if you want to take a closer look, but their descriptions you can find on the wiki for the repository.
</aside>
</section><section class="slide level2">

<h3 id="machine-learning">Machine learning</h3>
<p>Here is a list (incomplete) of what we tried:</p>
<ul>
<li>Random Forests
<ul>
<li>Random forest classifiers</li>
<li>Totally random tree embedding</li>
<li>Extra-tree feature selection</li>
</ul></li>
<li>Support Vector Machines
<ul>
<li>Various different kernels</li>
</ul></li>
<li>Logistic Regression</li>
<li>Adaboost</li>
<li>Platt scaling</li>
<li>Univariate feature selection</li>
<li>Restricted Boltzmann machine</li>
<li>Recursive feature elimination</li>
<li>...</li>
</ul>
</section><section class="slide level2">

<h3 id="organising-the-project">Organising the project</h3>
<ul>
<li>Teamwork with git experience</li>
<li>TDD</li>
<li>Code documentation</li>
</ul>
<aside class="notes">
<p>Easy to use it as a opportunity to get acquainted with a new technique you might want to use in another project. You can quickly understand how to make your chosen ML algorithm work well, as you see the results right away, and it is a real problem.</p>
<p>Scott mainly worked on the preprocessing in matlab. In the repository there is a vast directory of matlab scripts, which I avoid.</p>
We were also able to put some time into learning development techniques which we wouldn't find a good excuse to look at otherwise. This was largely using unit tests in Python to get some bugs out of the code we were using to build training and test sets from the processed HDF5s.
</aside>
</section></section>
<section><section id="tips-and-tricks-in-seizure-prediction" class="titleslide slide level1"><h1>Tips and tricks in seizure prediction</h1></section><section id="our-process" class="slide level2">
<h1>Our process</h1>
<figure>
<img src="images/hsflow_scaled.svg" alt="Our data flow chart." /><figcaption>Our data flow chart.</figcaption>
</figure>
<aside class="notes">
<p>We did our preprocessing in Matlab and it seemed like the best way to get this data from Matlab into Python would be to write it to HDF5s for each preprocessing operation and load them into Python as required.</p>
<p>Once Scott was done with it this resulted in around 300GB of HDF5 files from around 30GB of raw data.</p>
<p>Figuring our which of these were actually going to be useful was a massive problem, considering we only had around 4000 samples spread over 7 subjects.</p>
<p>We launched several batch scripts with the hope that we might be able to find a &quot;silver bullet&quot; feature somewhere in there.</p>
This failed, and it proved extremely difficult to find anything better than our hand-picked set of features we chose at the start of classification, once the preprocessing had been finished.
</aside>
</section><section id="model-averaging" class="slide level2">
<h1>Model averaging</h1>
<figure>
<img src="images/yeggsdaffy.png" />
</figure>
<aside class="notes">
<p>So we were 10 hours from the deadline and I had completely run out of ideas. I'd been trying to improve our score all week with various different forms of feature selection and including other preprocessed features to try to improve it and had turned up nothing. Finlay and Scott had been doing the same thing, and we'd come up with nothing.</p>
<p>In a last ditch attempt to improve our score slightly I just took our two best submission csvs and averaged the predictions.</p>
<p>We immediately jumped up 4 places.</p>
Including some other high performing submissions we were able to jump up several more places before the end of the competition.
</aside>
</section><section id="genetic-algorithms" class="slide level2">
<h1>Genetic algorithms</h1>
<p>Michael Hill's method came up <a href="https://github.com/MichaelHills/seizure-prediction">on Github</a> two days ago <span class="citation" data-cites="hill">(Hill, 2014)</span>:</p>
<blockquote>
<p>...population size of 30 and runs for 10 generations. The population is initialised with random feature masks consisting of roughly 55% features activated and the other 45% masked away. The fitness function is simply a CV ROC AUC score.</p>
</blockquote>
<p>His model:</p>
<div class="fragment">
<blockquote>
<p>The default selected classifier for submission is linear regression.</p>
</blockquote>
<aside class="notes">
<p>Michael Hill won the previous AES seizure challenge. This time he came 5th, and the method he used was this complicated genetic approach to feature selection.</p>
<p>Linear regression was the method of choice a.k.a. Scikit-learn linear regression with the <code>predict_proba</code> method scaled and sigmoided. Others with high-scoring results also did this, including Jonathan Tapson.</p>
</aside>
</div>
</section><section id="repository" class="slide level2">
<h1>Repository</h1>
<p>Our repository can be found at: https://github.com/Neuroglycerin/hail-seizure</p>
</section></section>
<section><section id="competitive-data-science" class="titleslide slide level1"><h1>Competitive Data Science</h1></section><section id="conclusions" class="slide level2">
<h1>Conclusions</h1>
<div class="fragment">
<p>Advantages:</p>
<ul>
<li>Get to try new things</li>
<li>Learn new skills</li>
<li>Working break from your PhD - you get <em>immediate feedback</em></li>
<li>Might discover something useful</li>
</ul>
</div>
<div class="fragment">
<p>Disadvantages:</p>
<ul>
<li>Can quickly absorb time</li>
<li>You have to have a good team</li>
<li>Models people create are not necessarily useful:
<ul>
<li>Netflix challenge</li>
<li>Engineered ensemble models are over-complicated</li>
</ul></li>
</ul>
</div>
</section><section id="next" class="slide level2">
<h1>Next?</h1>
<p>The next competitions coming up are:</p>
<ul>
<li>BCI Challenge @ NER 2015 - $1,000</li>
<li>Helping Santa's Helpers - $20,000</li>
<li>Click-Through Rate Prediction - $15,000</li>
</ul>
<aside class="notes">
<p>If you want to start one now, here are your options.</p>
<p>The first of these is probably the interesting one. The goal is to detect errors in a spelling task, given EEG recordings.</p>
<p>The second involves optimising an objective function by assigning different elves to different toys. Unfortunately, it's by a company called FICO and they want you to use their special software to do it...</p>
The third you're allowed to use whatever you want, but it's not very interesting. And if you're doing a PhD you probably never wanted to work for an advertising company.
</aside>
<p>Some text.</p>
</section><section id="references" class="slide level2">
<h1>References</h1>
<!---
Hill M (2014) Github - michaelHills/seizure-prediction. Available from: https://github.com/MichaelHills/seizure-prediction.

Kaggle Inc (2014) American epilepsy society seizure prediction challenge. Available from: https://www.kaggle.com/c/seizure-prediction.
--->

<!---[repo]:  --->


<div class="references">
<p>Hill M (2014) Github - michaelHills/seizure-prediction. Available from: <a href="https://github.com/MichaelHills/seizure-prediction">https://github.com/MichaelHills/seizure-prediction</a>.</p>
<p><span>Kaggle Inc</span> (2014) American epilepsy society seizure prediction challenge. Available from: <a href="https://www.kaggle.com/c/seizure-prediction">https://www.kaggle.com/c/seizure-prediction</a>.</p>
<p>Queiroz CM, Gorter JA, Silva FHL da, et al. (2009) Dynamics of evoked local field potentials in the hippocampus of epileptic rats with spontaneous seizures. <em>Journal of Neurophysiology</em>, 101(3), 1588–1597, Available from: <a href="http://jn.physiology.org/content/101/3/1588">http://jn.physiology.org/content/101/3/1588</a> (accessed 28 November 2014).</p>
</div>
</section></section>
    </div>
  </div>


  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.min.js"></script>

  <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        theme: 'solarized', // available themes are in /css/theme
        transition: 'linear', // default/cube/page/concave/zoom/linear/fade/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
//          { src: 'reveal.js/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; }, }
//          { src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
]});
    </script>
    </body>
</html>
