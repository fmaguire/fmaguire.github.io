<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Finlay Maguire" />
  <title>Stumbling around the decision boundary</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="../reveal.js/css/reveal.min.css"/>
    <style type="text/css">code{white-space: pre;}</style>
    <link rel="stylesheet" href="../reveal.js/css/theme/simple.css" id="theme">
    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = '../reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>
    <!--[if lt IE 9]>
    <script src="../reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Stumbling around the decision boundary</h1>
    <h2 class="author">Finlay Maguire</h2>
    <h3 class="date">root@finlaymagui.re</h3>
</section>

<section><section id="getting-into-machine-learning" class="titleslide slide level1"><h1>Getting into Machine Learning</h1></section></section>
<section><section id="overview" class="titleslide slide level1"><h1>Overview</h1></section><section class="slide level2">

<ul>
<li>What is my background</li>
<li>How I got into ML</li>
<li>Overview of ways I've used ML</li>
<li>What I wish I had known earlier</li>
</ul>
</section></section>
<section><section id="who-am-i" class="titleslide slide level1"><h1>Who Am I?</h1></section><section id="background" class="slide level2">
<h1>Background</h1>
<ul>
<li>High school &quot;computing&quot; and maths</li>
<li>Bioscience Undergraduate: insigificant courses and research project</li>
<li>Finishing Bioinformatics PhD looking</li>
<li>So no significant formal training in ML, maths or computer science</li>
</ul>
</section></section>
<section><section id="getting-into-ml" class="titleslide slide level1"><h1>Getting into ML</h1></section><section id="moocs" class="slide level2">
<h1>MOOCs!</h1>
<ul>
<li><a href="https://www.coursera.org/course/matrix">Philip Klein's (Brown) &quot;Coding the Matrix&quot;</a> *</li>
<li><a href="https://class.coursera.org/ml-005/lecture/preview">Andrew Ng's (Stanford) &quot;Machine Learning&quot;</a> *</li>
<li><a href="https://work.caltech.edu/telecourse.html">Yaser Abu-Mostafa's (Caltech) &quot;Learning from Data&quot;</a></li>
<li><a href="https://www.coursera.org/course/linearprogramming">Sriram Sankaranarayanan &amp; Shalom D. Ruben's (UCBoulder) &quot;Linear and Integer Programming&quot;</a></li>
<li><a href="https://www.coursera.org/course/pgm">Daphne Koller's (Standford) &quot;Probabilistic Graphical Models&quot;</a></li>
</ul>
</section><section id="textbooks" class="slide level2">
<h1>Textbooks</h1>
<figure>
<img src="../assets/presentation/GIML/books.png" />
</figure>
<ul>
<li>Christopher Bishop's &quot;Pattern Recognition and Machine Learning&quot;</li>
<li>Kevin Murphy's &quot;Machine Learning: A Probabilistic Perspective&quot;</li>
<li>Gilbert Strang's &quot;Linear Algebra and Its Applications&quot;</li>
</ul>
</section><section id="practice" class="slide level2">
<h1>Practice</h1>
<figure>
<img src="../assets/presentation/GIML/kaggle.png" />
</figure>
<ul>
<li>Provided Dataset</li>
<li>Evaluation Metric</li>
<li>Public-Private scoreboards</li>
<li>Top N winners methods and code are released publically (on Kaggle's <a href="http://blog.kaggle.com/">No Free Hunch Blog</a>)</li>
</ul>
</section><section id="warning" class="slide level2">
<h1>Warning</h1>
<figure>
<img src="../assets/presentation/GIML/hspunchcard.png" alt="Kaggle can and will eat your time" /><figcaption>Kaggle can and will eat your time</figcaption>
</figure>
</section><section id="other-media" class="slide level2">
<h1>Other media</h1>
<ul>
<li><a href="http://fastml.com/">Zygmunt Zajac's FastML Blog</a></li>
<li><a href="http://karpathy.github.io/">Andrej Karpathy's Blog (DNNs)</a></li>
<li><a href="https://nips.cc">Neural Information Processing Systems workshops</a></li>
<li><a href="http://www.thetalkingmachines.com">Ryan Adams' and Katherine Gorman's Talking Machines Podcast</a></li>
<li><a href="http://scikit-learn.org/stable/">Scikit-Learn Documentation</a></li>
<li><a href="http://stats.stackexchange.com/">Cross Validated - StackOverflow Q&amp;A System for ML and stats</a></li>
</ul>
</section></section>
<section><section id="projects" class="titleslide slide level1"><h1>Projects</h1></section></section>
<section><section id="parkour" class="titleslide slide level1"><h1>parKour</h1></section><section class="slide level2">

<figure>
<img src="../assets/presentation/GIML/gc.png" alt="Metatranscriptome GC %" /><figcaption>Metatranscriptome GC %</figcaption>
</figure>
<ul>
<li>400M 150bp PE reads (159GB)</li>
</ul>
</section><section class="slide level2">

<ul>
<li>K-means clustering</li>
<li>Fit GMM using Expectation-Maximisation</li>
<li>Python (SKLearn) still hadn't finished PARSING input after 48 hours</li>
<li>C++ (MLPACK/ARMADILLO): 12 hours (6GB of memory) single threaded</li>
<li>Lesson: use the right tool for the job (Python: Scikit-Learn, R: Caret, Java: Weka, C++: MLPACK etc)</li>
</ul>
</section></section>
<section><section id="hail-seizure" class="titleslide slide level1"><h1>Hail-Seizure</h1></section><section class="slide level2">

<figure>
<img src="../assets/presentation/GIML/hs_dog.png" alt="American Epilepsy Society Seizure Prediction Challenge" /><figcaption>American Epilepsy Society Seizure Prediction Challenge</figcaption>
</figure>
</section><section class="slide level2">

<figure>
<img src="../assets/presentation/GIML/eeg.png" alt="iEEGs" /><figcaption>iEEGs</figcaption>
</figure>
</section><section class="slide level2">

<figure>
<img src="../assets/presentation/GIML/roc.png" alt="true positives (sensitivity) vs false negatives (decreased specificity) (source)" /><figcaption>true positives (sensitivity) vs false negatives (decreased specificity) (<a href="http://www.sprawls.org/ppmi2/IMGCHAR/">source</a>)</figcaption>
</figure>
</section><section class="slide level2">

<figure>
<img src="../assets/presentation/GIML/hsflow.png" />
</figure>
</section><section id="data-processing-and-feature-extraction" class="slide level2">
<h1>Data processing and feature extraction</h1>
<ul>
<li>Data preprocessing: downsampling, cleaning</li>
<li>Channel correlations: independent component analysis, common spatial patterns, MVARs</li>
<li>Approximately 850 different features</li>
<li>Recursive Feature Elimination</li>
</ul>
</section><section id="machine-learning" class="slide level2">
<h1>Machine learning</h1>
<ul>
<li>Random forests</li>
<li>Support Vector Machines</li>
<li>Logistic Regression</li>
<li>Adaboost</li>
<li>Ensembles!</li>
</ul>
</section><section id="performance" class="slide level2">
<h1>Performance</h1>
<ul>
<li>Top 5% (16/504)</li>
<li>Reasonable accurate prediction - 0.77014 AUC</li>
<li>Team: Gavin Gray and Scott Lowe</li>
<li>Lesson: planning and ensembles</li>
</ul>
</section></section>
<section><section id="dendrogenous" class="titleslide slide level1"><h1>Dendrogenous</h1></section><section class="slide level2">

<figure>
<img src="../assets/presentation/GIML/pbmr.jpg" alt="Paramecium bursaria with Micractinium reisseri endosymbiont" /><figcaption><em>Paramecium bursaria</em> with <em>Micractinium reisseri</em> endosymbiont</figcaption>
</figure>
</section><section class="slide level2">

<ul>
<li>Metatranscriptome origin classification</li>
<li>Goal: classify transcripts into origin species</li>
</ul>
<figure>
<img src="../assets/presentation/GIML/binning.png" alt="Phylogenetics vs top BLAST hits" /><figcaption>Phylogenetics vs top BLAST hits</figcaption>
</figure>
</section><section class="slide level2">

<ul>
<li>Features: Phylogenetic tree and sequence features (GC, trinucleotide)</li>
<li>SVM/RVM</li>
<li>F1 Score nearly as good as manual (ad hoc self-consistency...)</li>
</ul>
</section></section>
<section><section id="neukrill-net" class="titleslide slide level1"><h1>NeuKrill-Net</h1></section><section class="slide level2">

<figure>
<img src="../assets/presentation/NDSB/raw_data.png" alt="Image classification of plankton" /><figcaption>Image classification of plankton</figcaption>
</figure>
</section></section>
<section><section id="eyes-have-it" class="titleslide slide level1"><h1>Eyes-Have-It</h1></section><section class="slide level2">

<figure>
<img src="../assets/presentation/GIML/retina.jpg" />
</figure>
<ul>
<li>Diabetic retinopathy detection</li>
<li>Two eyes, 5 ratings for each eye</li>
<li>Same ML approach as NeuKrill-Net</li>
<li>Difficult cost function (quadratic negative kappa)</li>
</ul>
</section></section>
<section><section id="spearseq" class="titleslide slide level1"><h1>Spearseq</h1></section><section class="slide level2">

<figure>
<img src="../assets/presentation/GIML/spearseq.png" />
</figure>
</section><section class="slide level2">

<ul>
<li><em>de novo</em> assembly parameter optimisation (k-mer size, minimum coverage, normalisation)</li>
<li>Bayesian optimisation of assembly likelihood</li>
<li>Spearmint experimentation evaluated using RSEM-eval</li>
<li>Key challenges i.e. representative subset, generalising for assemblers</li>
</ul>
</section></section>
<section><section id="awedify" class="titleslide slide level1"><h1>Awedify</h1></section><section class="slide level2">

<ul>
<li>Short-form spoken word recommender system</li>
<li>Goal: Recommender system for audioclips</li>
<li>Hare brained startup</li>
<li>Audiotranscription and metadata (project Gutenberg)</li>
<li>Very much a work in progress</li>
<li>RNN and LSTM are really cool</li>
</ul>
</section></section>
<section><section id="things-i-wish-i-had-known-or-understood-earlier" class="titleslide slide level1"><h1>Things I wish I had known or understood earlier!</h1></section><section id="plot-everything" class="slide level2">
<h1>Plot everything</h1>
<ul>
<li>Exploratory data analysis</li>
<li>Plot density, scatter, structure of data</li>
</ul>
</section><section class="slide level2">

<figure>
<img src="../assets/presentation/GIML/tsne_class.png" alt="t-SNE of EEG features" /><figcaption>t-SNE of EEG features</figcaption>
</figure>
</section><section id="no-free-lunch" class="slide level2">
<h1>No Free Lunch</h1>
<ul>
<li>&quot;any two [...] algorithms are equivalent when their performance is averaged across all possible problems&quot;</li>
<li>No universally best optimiser, classifier, or metric</li>
<li>Wolpert and McCreedy proofs</li>
<li>Real world not so nicely random</li>
</ul>
</section><section id="ml-method-evaluation-optimisation" class="slide level2">
<h1>ML = Method + Evaluation + Optimisation</h1>
<ul>
<li>The ML algorithm (e.g. KNN, SVM, RF etc) is only part of the problem</li>
<li>Important to be aware of evaluation (e.g. sum of squares distance, error)</li>
<li>and optimisation method (e.g. gradient descent, expectation-maximisation)</li>
<li>Can make a big difference to performance (and run time)</li>
<li>Ties into NFL theory</li>
</ul>
</section><section id="curse-of-dimensionality" class="slide level2">
<h1>Curse of dimensionality</h1>
<figure>
<img src="../assets/presentation/GIML/bengio_dim.jpg" alt="from Bengio&#39;s homepage" /><figcaption>from Bengio's <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/research.html">homepage</a></figcaption>
</figure>
<ul>
<li>Intuitions will fail in high dimensions</li>
</ul>
</section><section class="slide level2">

<figure>
<img src="../assets/presentation/GIML/manifold.png" alt="Blessing of non-uniformity (from SKLearn Documentation)" /><figcaption>Blessing of non-uniformity (from <a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html">SKLearn Documentation</a>)</figcaption>
</figure>
</section><section id="highly-iterative-process" class="slide level2">
<h1>Highly iterative process</h1>
<ul>
<li>Most of the work is the boring stuff</li>
<li>Data exploration</li>
<li>Data gathering</li>
<li>Data cleaning</li>
<li>More data can beat smarter algorithms (but not always)</li>
</ul>
</section><section class="slide level2">

<ul>
<li>Worth taking time setting up tools and data</li>
<li><em>UNIT TESTS</em></li>
<li>Literate programming e.g. Rmd, Jupyter</li>
<li>Version control e.g. git</li>
</ul>
</section><section id="if-in-doubt-combine-models" class="slide level2">
<h1>If in doubt combine models!</h1>
<ul>
<li>Bagging (average estimators trained on random subsets of data)</li>
<li>Merging (model averaging)</li>
<li>Stacking (feed estimators into other estimators)</li>
<li>Boosting (iteratively train estimators on data that previous models misclassify)</li>
</ul>
</section></section>
<section><section id="conclusion" class="titleslide slide level1"><h1>Conclusion</h1></section><section class="slide level2">

<ul>
<li>Machine learning can be used to do cool things</li>
<li>It is not as opaque as it appears</li>
<li>Optimisation and Evaluation is as important as ML algorithm</li>
<li>Highly iterative process</li>
<li>Use version control and literate programming to save yourself a lot of difficulty</li>
</ul>
</section></section>
    </div>
  </div>


  <script src="../reveal.js/lib/js/head.min.js"></script>
  <script src="../reveal.js/js/reveal.min.js"></script>

  <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        theme: 'moon', // available themes are in /css/theme
        transition: 'none', // default/cube/page/concave/zoom/linear/fade/none

        // Optional libraries used to extend on ../reveal.js
        dependencies: [
          { src: '../reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: '../reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: '../reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
//          { src: '../reveal.js/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; }, }
//          { src: '../reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
]});
    </script>
    </body>
</html>
